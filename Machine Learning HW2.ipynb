{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb60c25-0806-4930-9c8f-2df6d593b6f0",
   "metadata": {},
   "source": [
    "# Machine Learning HW2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e825f86-27e0-4d1e-ac1e-97750d726d3a",
   "metadata": {},
   "source": [
    "### FFNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6a823-9686-43cb-bf5d-d923a483a0c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from argparse import ArgumentParser\n",
    "import sys\n",
    "\n",
    "\n",
    "unk = '<UNK>'\n",
    "# Consult the PyTorch documentation for information on the functions used below:\n",
    "# https://pytorch.org/docs/stable/torch.html\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim, h):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.h = h\n",
    "        self.W1 = nn.Linear(input_dim, h)\n",
    "        self.activation = nn.ReLU() # The rectified linear unit; one valid choice of activation function\n",
    "        self.output_dim = 5\n",
    "        self.W2 = nn.Linear(h, self.output_dim)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax() # The softmax function that converts vectors into probability distributions; computes log probabilities for computational benefits\n",
    "        self.loss = nn.NLLLoss() # The cross-entropy/negative log likelihood loss taught in class\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        return self.loss(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        # Obtain first hidden layer representation\n",
    "        hidden_rep = self.activation(self.W1(input_vector))\n",
    "\n",
    "        # Obtain output layer representation\n",
    "        output_rep = self.W2(hidden_rep)\n",
    "\n",
    "        # Obtain probability distribution\n",
    "        predicted_vector = self.softmax(output_rep)\n",
    "\n",
    "        return predicted_vector\n",
    "\n",
    "\n",
    "# Returns: \n",
    "# vocab = A set of strings corresponding to the vocabulary\n",
    "def make_vocab(data):\n",
    "    vocab = set()\n",
    "    for document, _ in data:\n",
    "        for word in document:\n",
    "            vocab.add(word)\n",
    "    return vocab \n",
    "\n",
    "\n",
    "# Returns:\n",
    "# vocab = A set of strings corresponding to the vocabulary including <UNK>\n",
    "# word2index = A dictionary mapping word/token to its index (a number in 0, ..., V - 1)\n",
    "# index2word = A dictionary inverting the mapping of word2index\n",
    "def make_indices(vocab):\n",
    "    vocab_list = sorted(vocab)\n",
    "    vocab_list.append(unk)\n",
    "    word2index = {}\n",
    "    index2word = {}\n",
    "    for index, word in enumerate(vocab_list):\n",
    "        word2index[word] = index \n",
    "        index2word[index] = word \n",
    "    vocab.add(unk)\n",
    "    return vocab, word2index, index2word \n",
    "\n",
    "\n",
    "# Returns:\n",
    "# vectorized_data = A list of pairs (vector representation of input, y)\n",
    "def convert_to_vector_representation(data, word2index):\n",
    "    vectorized_data = []\n",
    "    for document, y in data:\n",
    "        vector = torch.zeros(len(word2index)) \n",
    "        for word in document:\n",
    "            index = word2index.get(word, word2index[unk])\n",
    "            vector[index] += 1\n",
    "        vectorized_data.append((vector, y))\n",
    "    return vectorized_data\n",
    "\n",
    "\n",
    "\n",
    "def load_data(train_data, val_data):\n",
    "    with open(train_data) as training_f:\n",
    "        training = json.load(training_f)\n",
    "    with open(val_data) as valid_f:\n",
    "        validation = json.load(valid_f)\n",
    "\n",
    "    tra = []\n",
    "    val = []\n",
    "    for elt in training:\n",
    "        tra.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
    "    for elt in validation:\n",
    "        val.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
    "\n",
    "    return tra, val\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-hd\", \"--hidden_dim\", type=int, required = True, help = \"hidden_dim\")\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, required = True, help = \"num of epochs to train\")\n",
    "    parser.add_argument(\"--train_data\", required = True, help = \"path to training data\")\n",
    "    parser.add_argument(\"--val_data\", required = True, help = \"path to validation data\")\n",
    "    parser.add_argument(\"--test_data\", default = \"to fill\", help = \"path to test data\")\n",
    "    parser.add_argument('--do_train', action='store_true')\n",
    "    \n",
    "    # Check if running in an interactive environment\n",
    "    if 'ipykernel' in sys.modules or 'spyder' in sys.modules:\n",
    "        args = parser.parse_args(args=[\"--hidden_dim\", \"128\", \"--epochs\", \"10\", \"--train_data\", \"training.json\", \"--val_data\", \"validation.json\"])\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    # fix random seeds\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # load data\n",
    "    print(\"========== Loading data ==========\")\n",
    "    train_data, valid_data = load_data(args.train_data, args.val_data) # X_data is a list of pairs (document, y); y in {0,1,2,3,4}\n",
    "    vocab = make_vocab(train_data)\n",
    "    vocab, word2index, index2word = make_indices(vocab)\n",
    "\n",
    "    print(\"========== Vectorizing data ==========\")\n",
    "    train_data = convert_to_vector_representation(train_data, word2index)\n",
    "    valid_data = convert_to_vector_representation(valid_data, word2index)\n",
    "    \n",
    "    model = FFNN(input_dim = len(vocab), h = args.hidden_dim)\n",
    "    optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.9)\n",
    "    print(\"========== Training for {} epochs ==========\".format(args.epochs))\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = None\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "        print(\"Training started for epoch {}\".format(epoch + 1))\n",
    "        random.shuffle(train_data) # Good practice to shuffle order of training data\n",
    "        minibatch_size = 16\n",
    "        N = len(train_data) \n",
    "        for minibatch_index in tqdm(range(N // minibatch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            loss = None\n",
    "            for example_index in range(minibatch_size):\n",
    "                input_vector, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n",
    "                predicted_vector = model(input_vector)\n",
    "                predicted_label = torch.argmax(predicted_vector)\n",
    "                correct += int(predicted_label == gold_label)\n",
    "                total += 1\n",
    "                example_loss = model.compute_Loss(predicted_vector.view(1,-1), torch.tensor([gold_label]))\n",
    "                if loss is None:\n",
    "                    loss = example_loss\n",
    "                else:\n",
    "                    loss += example_loss\n",
    "            loss = loss / minibatch_size\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Training completed for epoch {}\".format(epoch + 1))\n",
    "        print(\"Training accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
    "        print(\"Training time for this epoch: {}\".format(time.time() - start_time))\n",
    "\n",
    "        # Write results to output file\n",
    "        with open(\"training_results.out\", \"a\") as file:\n",
    "            file.write(f\"Epoch {epoch + 1} Training Accuracy: {correct / total}\\n\")\n",
    "            file.write(f\"Epoch {epoch + 1} Training Time: {time.time() - start_time}\\n\")\n",
    "\n",
    "        loss = None\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "        print(\"Validation started for epoch {}\".format(epoch + 1))\n",
    "        minibatch_size = 16\n",
    "        N = len(valid_data) \n",
    "        for minibatch_index in tqdm(range(N // minibatch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            loss = None\n",
    "            for example_index in range(minibatch_size):\n",
    "                input_vector, gold_label = valid_data[minibatch_index * minibatch_size + example_index]\n",
    "                predicted_vector = model(input_vector)\n",
    "                predicted_label = torch.argmax(predicted_vector)\n",
    "                correct += int(predicted_label == gold_label)\n",
    "                total += 1\n",
    "                example_loss = model.compute_Loss(predicted_vector.view(1,-1), torch.tensor([gold_label]))\n",
    "                if loss is None:\n",
    "                    loss = example_loss\n",
    "                else:\n",
    "                    loss += example_loss\n",
    "            loss = loss / minibatch_size\n",
    "        print(\"Validation completed for epoch {}\".format(epoch + 1))\n",
    "        print(\"Validation accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
    "        print(\"Validation time for this epoch: {}\".format(time.time() - start_time))\n",
    "\n",
    "        # Write results to output file\n",
    "        with open(\"validation_results.out\", \"a\") as file:\n",
    "            file.write(f\"Epoch {epoch + 1} Validation Accuracy: {correct / total}\\n\")\n",
    "            file.write(f\"Epoch {epoch + 1} Validation Time: {time.time() - start_time}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ca22a-28b3-4272-961a-0589f12c0509",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b607696-fc40-488a-9016-7a682eca010e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import string\n",
    "from argparse import ArgumentParser\n",
    "import pickle\n",
    "\n",
    "unk = '<UNK>'\n",
    "# Consult the PyTorch documentation for information on the functions used below:\n",
    "# https://pytorch.org/docs/stable/torch.html\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, h):  # Add relevant parameters\n",
    "        super(RNN, self).__init__()\n",
    "        self.h = h\n",
    "        self.numOfLayer = 1\n",
    "        self.rnn = nn.RNN(input_dim, h, self.numOfLayer, nonlinearity='tanh')\n",
    "        self.W = nn.Linear(h, 5)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.loss = nn.NLLLoss()\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        return self.loss(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, inputs): \n",
    "        # Obtain hidden layer representation \n",
    "        output, hidden = self.rnn(inputs) \n",
    "\n",
    "        # Obtain output layer representations \n",
    "        output_rep = self.W(output) \n",
    "\n",
    "        # Sum over the output representations\n",
    "        summed_output_rep = torch.sum(output_rep, dim=0) \n",
    "\n",
    "        # Obtain probability distribution \n",
    "        predicted_vector = self.softmax(summed_output_rep) \n",
    "        \n",
    "        return predicted_vector\n",
    "\n",
    "\n",
    "def load_data(train_data, val_data):\n",
    "    with open(train_data) as training_f:\n",
    "        training = json.load(training_f)\n",
    "    with open(val_data) as valid_f:\n",
    "        validation = json.load(valid_f)\n",
    "\n",
    "    tra = []\n",
    "    val = []\n",
    "    for elt in training:\n",
    "        tra.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
    "    for elt in validation:\n",
    "        val.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
    "    return tra, val\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-hd\", \"--hidden_dim\", type=int, required = True, help = \"hidden_dim\")\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, required = True, help = \"num of epochs to train\")\n",
    "    parser.add_argument(\"--train_data\", required = True, help = \"path to training data\")\n",
    "    parser.add_argument(\"--val_data\", required = True, help = \"path to validation data\")\n",
    "    parser.add_argument(\"--test_data\", default = \"to fill\", help = \"path to test data\")\n",
    "    parser.add_argument('--do_train', action='store_true')\n",
    "    args = parser.parse_args(args=[\"--hidden_dim\", '256', \"--epochs\", '10', \"--train_data\", \"training.json\", \"--val_data\",\n",
    "\"validation.json\"])\n",
    "\n",
    "    print(\"========== Loading data ==========\")\n",
    "    train_data, valid_data = load_data(args.train_data, args.val_data) # X_data is a list of pairs (document, y); y in {0,1,2,3,4}\n",
    "\n",
    "    # Think about the type of function that an RNN describes. To apply it, you will need to convert the text data into vector representations.\n",
    "    # Further, think about where the vectors will come from. There are 3 reasonable choices:\n",
    "    # 1) Randomly assign the input to vectors and learn better embeddings during training; see the PyTorch documentation for guidance\n",
    "    # 2) Assign the input to vectors using pretrained word embeddings. We recommend any of {Word2Vec, GloVe, FastText}. Then, you do not train/update these embeddings.\n",
    "    # 3) You do the same as 2) but you train (this is called fine-tuning) the pretrained embeddings further.\n",
    "    # Option 3 will be the most time consuming, so we do not recommend starting with this\n",
    "\n",
    "    print(\"========== Vectorizing data ==========\")\n",
    "    model = RNN(50, args.hidden_dim)  # Fill in parameters\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    word_embedding = pickle.load(open('./word_embedding.pkl', 'rb'))\n",
    "\n",
    "    stopping_condition = False\n",
    "    epoch = 0\n",
    "\n",
    "    last_train_accuracy = 0\n",
    "    last_validation_accuracy = 0\n",
    "\n",
    "    while not stopping_condition:\n",
    "        random.shuffle(train_data)\n",
    "        model.train()\n",
    "        # You will need further code to operationalize training, ffnn.py may be helpful\n",
    "        print(\"Training started for epoch {}\".format(epoch + 1))\n",
    "        train_data = train_data\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        minibatch_size = 16\n",
    "        N = len(train_data)\n",
    "\n",
    "        loss_total = 0\n",
    "        loss_count = 0\n",
    "        for minibatch_index in tqdm(range(N // minibatch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            loss = None\n",
    "            for example_index in range(minibatch_size):\n",
    "                input_words, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n",
    "                input_words = \" \".join(input_words)\n",
    "\n",
    "                # Remove punctuation\n",
    "                input_words = input_words.translate(input_words.maketrans(\"\", \"\", string.punctuation)).split()\n",
    "\n",
    "                # Look up word embedding dictionary\n",
    "                vectors = [word_embedding[i.lower()] if i.lower() in word_embedding.keys() else word_embedding['unk'] for i in input_words ]\n",
    "\n",
    "                # Transform the input into required shape\n",
    "                vectors = torch.tensor(vectors).view(len(vectors), 1, -1)\n",
    "                output = model(vectors)\n",
    "\n",
    "                # Get loss\n",
    "                example_loss = model.compute_Loss(output.view(1,-1), torch.tensor([gold_label]))\n",
    "\n",
    "                # Get predicted label\n",
    "                predicted_label = torch.argmax(output)\n",
    "\n",
    "                correct += int(predicted_label == gold_label)\n",
    "                # print(predicted_label, gold_label)\n",
    "                total += 1\n",
    "                if loss is None:\n",
    "                    loss = example_loss\n",
    "                else:\n",
    "                    loss += example_loss\n",
    "\n",
    "            loss = loss / minibatch_size\n",
    "            loss_total += loss.data\n",
    "            loss_count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss_total/loss_count)\n",
    "        print(\"Training completed for epoch {}\".format(epoch + 1))\n",
    "        print(\"Training accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
    "        \n",
    "        # Write results to output file\n",
    "        with open(\"training_results.out\", \"a\") as file:\n",
    "            file.write(f\"Epoch {epoch + 1} Training Accuracy: {correct / total}\\n\")\n",
    "            file.write(f\"Epoch {epoch + 1} Training Time: {time.time() - start_time}\\n\")\n",
    "            \n",
    "        trainning_accuracy = correct/total\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        random.shuffle(valid_data)\n",
    "        print(\"Validation started for epoch {}\".format(epoch + 1))\n",
    "        valid_data = valid_data\n",
    "\n",
    "        for input_words, gold_label in tqdm(valid_data):\n",
    "            input_words = \" \".join(input_words)\n",
    "            input_words = input_words.translate(input_words.maketrans(\"\", \"\", string.punctuation)).split()\n",
    "            vectors = [word_embedding[i.lower()] if i.lower() in word_embedding.keys() else word_embedding['unk'] for i\n",
    "                       in input_words]\n",
    "\n",
    "            vectors = torch.tensor(vectors).view(len(vectors), 1, -1)\n",
    "            output = model(vectors)\n",
    "            predicted_label = torch.argmax(output)\n",
    "            correct += int(predicted_label == gold_label)\n",
    "            total += 1\n",
    "            # print(predicted_label, gold_label)\n",
    "        print(\"Validation completed for epoch {}\".format(epoch + 1))\n",
    "        print(\"Validation accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
    "\n",
    "        # Write results to output file\n",
    "        with open(\"validation_results.out\", \"a\") as file:\n",
    "            file.write(f\"Epoch {epoch + 1} Validation Accuracy: {correct / total}\\n\")\n",
    "            file.write(f\"Epoch {epoch + 1} Validation Time: {time.time() - start_time}\\n\")\n",
    "            \n",
    "        validation_accuracy = correct/total\n",
    "\n",
    "        if validation_accuracy < last_validation_accuracy and trainning_accuracy > last_train_accuracy:\n",
    "            stopping_condition=True\n",
    "            print(\"Training done to avoid overfitting!\")\n",
    "            print(\"Best validation accuracy is:\", last_validation_accuracy)\n",
    "        else:\n",
    "            last_validation_accuracy = validation_accuracy\n",
    "            last_train_accuracy = trainning_accuracy\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "\n",
    "\n",
    "    # You may find it beneficial to keep track of training accuracy or training loss;\n",
    "\n",
    "    # Think about how to update the model and what this entails. Consider ffnn.py and the PyTorch documentation for guidance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
