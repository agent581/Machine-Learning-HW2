{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb60c25-0806-4930-9c8f-2df6d593b6f0",
   "metadata": {},
   "source": [
    "# Machine Learning HW2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e825f86-27e0-4d1e-ac1e-97750d726d3a",
   "metadata": {},
   "source": [
    "### FFNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cac6a823-9686-43cb-bf5d-d923a483a0c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Loading data ==========\n",
      "========== Vectorizing data ==========\n",
      "========== Training for 10 epochs ==========\n",
      "Training started for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [02:11<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 1\n",
      "Training accuracy for epoch 1: 0.530125\n",
      "Training time for this epoch: 131.6940095424652\n",
      "Validation started for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 17.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 1\n",
      "Validation accuracy for epoch 1: 0.5425\n",
      "Validation time for this epoch: 2.8951947689056396\n",
      "Training started for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [01:51<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 2\n",
      "Training accuracy for epoch 2: 0.585875\n",
      "Training time for this epoch: 111.20730185508728\n",
      "Validation started for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 19.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 2\n",
      "Validation accuracy for epoch 2: 0.59375\n",
      "Validation time for this epoch: 2.6130781173706055\n",
      "Training started for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [02:00<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 3\n",
      "Training accuracy for epoch 3: 0.614125\n",
      "Training time for this epoch: 120.4599380493164\n",
      "Validation started for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 19.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 3\n",
      "Validation accuracy for epoch 3: 0.5875\n",
      "Validation time for this epoch: 2.5382015705108643\n",
      "Training started for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [02:00<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 4\n",
      "Training accuracy for epoch 4: 0.63975\n",
      "Training time for this epoch: 120.24436330795288\n",
      "Validation started for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 4\n",
      "Validation accuracy for epoch 4: 0.56875\n",
      "Validation time for this epoch: 2.7273967266082764\n",
      "Training started for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [01:56<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 5\n",
      "Training accuracy for epoch 5: 0.66275\n",
      "Training time for this epoch: 116.3075122833252\n",
      "Validation started for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 17.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 5\n",
      "Validation accuracy for epoch 5: 0.605\n",
      "Validation time for this epoch: 2.869783878326416\n",
      "Training started for epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [01:58<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 6\n",
      "Training accuracy for epoch 6: 0.689\n",
      "Training time for this epoch: 118.7164876461029\n",
      "Validation started for epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 20.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 6\n",
      "Validation accuracy for epoch 6: 0.605\n",
      "Validation time for this epoch: 2.453601360321045\n",
      "Training started for epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [01:56<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 7\n",
      "Training accuracy for epoch 7: 0.72625\n",
      "Training time for this epoch: 116.46232199668884\n",
      "Validation started for epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 7\n",
      "Validation accuracy for epoch 7: 0.54\n",
      "Validation time for this epoch: 2.730081081390381\n",
      "Training started for epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [01:59<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 8\n",
      "Training accuracy for epoch 8: 0.7435\n",
      "Training time for this epoch: 119.70438933372498\n",
      "Validation started for epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 19.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 8\n",
      "Validation accuracy for epoch 8: 0.61\n",
      "Validation time for this epoch: 2.6285922527313232\n",
      "Training started for epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [02:05<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 9\n",
      "Training accuracy for epoch 9: 0.775\n",
      "Training time for this epoch: 125.74061226844788\n",
      "Validation started for epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 9\n",
      "Validation accuracy for epoch 9: 0.59375\n",
      "Validation time for this epoch: 2.7623159885406494\n",
      "Training started for epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [01:59<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 10\n",
      "Training accuracy for epoch 10: 0.797625\n",
      "Training time for this epoch: 119.03123784065247\n",
      "Validation started for epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 10\n",
      "Validation accuracy for epoch 10: 0.59375\n",
      "Validation time for this epoch: 2.666266918182373\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from argparse import ArgumentParser\n",
    "import sys\n",
    "\n",
    "\n",
    "unk = '<UNK>'\n",
    "# Consult the PyTorch documentation for information on the functions used below:\n",
    "# https://pytorch.org/docs/stable/torch.html\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim, h):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.h = h\n",
    "        self.W1 = nn.Linear(input_dim, h)\n",
    "        self.activation = nn.ReLU() # The rectified linear unit; one valid choice of activation function\n",
    "        self.output_dim = 5\n",
    "        self.W2 = nn.Linear(h, self.output_dim)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax() # The softmax function that converts vectors into probability distributions; computes log probabilities for computational benefits\n",
    "        self.loss = nn.NLLLoss() # The cross-entropy/negative log likelihood loss taught in class\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        return self.loss(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        # Obtain first hidden layer representation\n",
    "        hidden_rep = self.activation(self.W1(input_vector))\n",
    "\n",
    "        # Obtain output layer representation\n",
    "        output_rep = self.W2(hidden_rep)\n",
    "\n",
    "        # Obtain probability distribution\n",
    "        predicted_vector = self.softmax(output_rep)\n",
    "\n",
    "        return predicted_vector\n",
    "\n",
    "\n",
    "# Returns: \n",
    "# vocab = A set of strings corresponding to the vocabulary\n",
    "def make_vocab(data):\n",
    "    vocab = set()\n",
    "    for document, _ in data:\n",
    "        for word in document:\n",
    "            vocab.add(word)\n",
    "    return vocab \n",
    "\n",
    "\n",
    "# Returns:\n",
    "# vocab = A set of strings corresponding to the vocabulary including <UNK>\n",
    "# word2index = A dictionary mapping word/token to its index (a number in 0, ..., V - 1)\n",
    "# index2word = A dictionary inverting the mapping of word2index\n",
    "def make_indices(vocab):\n",
    "    vocab_list = sorted(vocab)\n",
    "    vocab_list.append(unk)\n",
    "    word2index = {}\n",
    "    index2word = {}\n",
    "    for index, word in enumerate(vocab_list):\n",
    "        word2index[word] = index \n",
    "        index2word[index] = word \n",
    "    vocab.add(unk)\n",
    "    return vocab, word2index, index2word \n",
    "\n",
    "\n",
    "# Returns:\n",
    "# vectorized_data = A list of pairs (vector representation of input, y)\n",
    "def convert_to_vector_representation(data, word2index):\n",
    "    vectorized_data = []\n",
    "    for document, y in data:\n",
    "        vector = torch.zeros(len(word2index)) \n",
    "        for word in document:\n",
    "            index = word2index.get(word, word2index[unk])\n",
    "            vector[index] += 1\n",
    "        vectorized_data.append((vector, y))\n",
    "    return vectorized_data\n",
    "\n",
    "\n",
    "\n",
    "def load_data(train_data, val_data):\n",
    "    with open(train_data) as training_f:\n",
    "        training = json.load(training_f)\n",
    "    with open(val_data) as valid_f:\n",
    "        validation = json.load(valid_f)\n",
    "\n",
    "    tra = []\n",
    "    val = []\n",
    "    for elt in training:\n",
    "        tra.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
    "    for elt in validation:\n",
    "        val.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
    "\n",
    "    return tra, val\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-hd\", \"--hidden_dim\", type=int, required = True, help = \"hidden_dim\")\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, required = True, help = \"num of epochs to train\")\n",
    "    parser.add_argument(\"--train_data\", required = True, help = \"path to training data\")\n",
    "    parser.add_argument(\"--val_data\", required = True, help = \"path to validation data\")\n",
    "    parser.add_argument(\"--test_data\", default = \"to fill\", help = \"path to test data\")\n",
    "    parser.add_argument('--do_train', action='store_true')\n",
    "    \n",
    "    # Check if running in an interactive environment\n",
    "    if 'ipykernel' in sys.modules or 'spyder' in sys.modules:\n",
    "        args = parser.parse_args(args=[\"--hidden_dim\", \"128\", \"--epochs\", \"10\", \"--train_data\", \"training.json\", \"--val_data\", \"validation.json\"])\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    # fix random seeds\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # load data\n",
    "    print(\"========== Loading data ==========\")\n",
    "    train_data, valid_data = load_data(args.train_data, args.val_data) # X_data is a list of pairs (document, y); y in {0,1,2,3,4}\n",
    "    vocab = make_vocab(train_data)\n",
    "    vocab, word2index, index2word = make_indices(vocab)\n",
    "\n",
    "    print(\"========== Vectorizing data ==========\")\n",
    "    train_data = convert_to_vector_representation(train_data, word2index)\n",
    "    valid_data = convert_to_vector_representation(valid_data, word2index)\n",
    "    \n",
    "    model = FFNN(input_dim = len(vocab), h = args.hidden_dim)\n",
    "    optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.9)\n",
    "    print(\"========== Training for {} epochs ==========\".format(args.epochs))\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = None\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "        print(\"Training started for epoch {}\".format(epoch + 1))\n",
    "        random.shuffle(train_data) # Good practice to shuffle order of training data\n",
    "        minibatch_size = 16\n",
    "        N = len(train_data) \n",
    "        for minibatch_index in tqdm(range(N // minibatch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            loss = None\n",
    "            for example_index in range(minibatch_size):\n",
    "                input_vector, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n",
    "                predicted_vector = model(input_vector)\n",
    "                predicted_label = torch.argmax(predicted_vector)\n",
    "                correct += int(predicted_label == gold_label)\n",
    "                total += 1\n",
    "                example_loss = model.compute_Loss(predicted_vector.view(1,-1), torch.tensor([gold_label]))\n",
    "                if loss is None:\n",
    "                    loss = example_loss\n",
    "                else:\n",
    "                    loss += example_loss\n",
    "            loss = loss / minibatch_size\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Training completed for epoch {}\".format(epoch + 1))\n",
    "        print(\"Training accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
    "        print(\"Training time for this epoch: {}\".format(time.time() - start_time))\n",
    "\n",
    "        # Write results to output file\n",
    "        with open(\"training_results.out\", \"a\") as file:\n",
    "            file.write(f\"Epoch {epoch + 1} Training Accuracy: {correct / total}\\n\")\n",
    "            file.write(f\"Epoch {epoch + 1} Training Time: {time.time() - start_time}\\n\")\n",
    "\n",
    "        loss = None\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "        print(\"Validation started for epoch {}\".format(epoch + 1))\n",
    "        minibatch_size = 16\n",
    "        N = len(valid_data) \n",
    "        for minibatch_index in tqdm(range(N // minibatch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            loss = None\n",
    "            for example_index in range(minibatch_size):\n",
    "                input_vector, gold_label = valid_data[minibatch_index * minibatch_size + example_index]\n",
    "                predicted_vector = model(input_vector)\n",
    "                predicted_label = torch.argmax(predicted_vector)\n",
    "                correct += int(predicted_label == gold_label)\n",
    "                total += 1\n",
    "                example_loss = model.compute_Loss(predicted_vector.view(1,-1), torch.tensor([gold_label]))\n",
    "                if loss is None:\n",
    "                    loss = example_loss\n",
    "                else:\n",
    "                    loss += example_loss\n",
    "            loss = loss / minibatch_size\n",
    "        print(\"Validation completed for epoch {}\".format(epoch + 1))\n",
    "        print(\"Validation accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
    "        print(\"Validation time for this epoch: {}\".format(time.time() - start_time))\n",
    "\n",
    "        # Write results to output file\n",
    "        with open(\"validation_results.out\", \"a\") as file:\n",
    "            file.write(f\"Epoch {epoch + 1} Validation Accuracy: {correct / total}\\n\")\n",
    "            file.write(f\"Epoch {epoch + 1} Validation Time: {time.time() - start_time}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ca22a-28b3-4272-961a-0589f12c0509",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b607696-fc40-488a-9016-7a682eca010e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Loading data ==========\n",
      "========== Vectorizing data ==========\n",
      "Training started for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [04:29<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3442)\n",
      "Training completed for epoch 1\n",
      "Training accuracy for epoch 1: 0.462625\n",
      "Validation started for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 800/800 [00:09<00:00, 84.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 1\n",
      "Validation accuracy for epoch 1: 0.4625\n",
      "Training started for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [04:41<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8192)\n",
      "Training completed for epoch 2\n",
      "Training accuracy for epoch 2: 0.4395\n",
      "Validation started for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 800/800 [00:09<00:00, 84.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 2\n",
      "Validation accuracy for epoch 2: 0.4225\n",
      "Training started for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [04:52<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2023)\n",
      "Training completed for epoch 3\n",
      "Training accuracy for epoch 3: 0.4235\n",
      "Validation started for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 800/800 [00:09<00:00, 85.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 3\n",
      "Validation accuracy for epoch 3: 0.4475\n",
      "Training started for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [04:41<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2099)\n",
      "Training completed for epoch 4\n",
      "Training accuracy for epoch 4: 0.425125\n",
      "Validation started for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 800/800 [00:08<00:00, 89.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 4\n",
      "Validation accuracy for epoch 4: 0.41625\n",
      "Training started for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [04:54<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2413)\n",
      "Training completed for epoch 5\n",
      "Training accuracy for epoch 5: 0.416375\n",
      "Validation started for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 800/800 [00:10<00:00, 73.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 5\n",
      "Validation accuracy for epoch 5: 0.41375\n",
      "Training started for epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|█████████████████████████████████████▎                                          | 233/500 [02:25<02:46,  1.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 124\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Transform the input into required shape\u001b[39;00m\n\u001b[0;32m    123\u001b[0m vectors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(vectors)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(vectors), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 124\u001b[0m output \u001b[38;5;241m=\u001b[39m model(vectors)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Get loss\u001b[39;00m\n\u001b[0;32m    127\u001b[0m example_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_Loss(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor([gold_label]))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[33], line 34\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs): \n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Obtain hidden layer representation \u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(inputs) \n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Obtain output layer representations \u001b[39;00m\n\u001b[0;32m     37\u001b[0m     output_rep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW(output) \n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:714\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 714\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_tanh(\n\u001b[0;32m    715\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    716\u001b[0m             hx,\n\u001b[0;32m    717\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights,\n\u001b[0;32m    718\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    719\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    720\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[0;32m    721\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m    722\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m    723\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[0;32m    724\u001b[0m         )\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\n\u001b[0;32m    727\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    728\u001b[0m             hx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[0;32m    736\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import string\n",
    "from argparse import ArgumentParser\n",
    "import pickle\n",
    "\n",
    "unk = '<UNK>'\n",
    "# Consult the PyTorch documentation for information on the functions used below:\n",
    "# https://pytorch.org/docs/stable/torch.html\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, h):  # Add relevant parameters\n",
    "        super(RNN, self).__init__()\n",
    "        self.h = h\n",
    "        self.numOfLayer = 1\n",
    "        self.rnn = nn.RNN(input_dim, h, self.numOfLayer, nonlinearity='tanh')\n",
    "        self.W = nn.Linear(h, 5)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.loss = nn.NLLLoss()\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        return self.loss(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, inputs): \n",
    "        # Obtain hidden layer representation \n",
    "        output, hidden = self.rnn(inputs) \n",
    "\n",
    "        # Obtain output layer representations \n",
    "        output_rep = self.W(output) \n",
    "\n",
    "        # Sum over the output representations\n",
    "        summed_output_rep = torch.sum(output_rep, dim=0) \n",
    "\n",
    "        # Obtain probability distribution \n",
    "        predicted_vector = self.softmax(summed_output_rep) \n",
    "        \n",
    "        return predicted_vector\n",
    "\n",
    "\n",
    "def load_data(train_data, val_data):\n",
    "    with open(train_data) as training_f:\n",
    "        training = json.load(training_f)\n",
    "    with open(val_data) as valid_f:\n",
    "        validation = json.load(valid_f)\n",
    "\n",
    "    tra = []\n",
    "    val = []\n",
    "    for elt in training:\n",
    "        tra.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
    "    for elt in validation:\n",
    "        val.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
    "    return tra, val\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-hd\", \"--hidden_dim\", type=int, required = True, help = \"hidden_dim\")\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, required = True, help = \"num of epochs to train\")\n",
    "    parser.add_argument(\"--train_data\", required = True, help = \"path to training data\")\n",
    "    parser.add_argument(\"--val_data\", required = True, help = \"path to validation data\")\n",
    "    parser.add_argument(\"--test_data\", default = \"to fill\", help = \"path to test data\")\n",
    "    parser.add_argument('--do_train', action='store_true')\n",
    "    args = parser.parse_args(args=[\"--hidden_dim\", '256', \"--epochs\", '10', \"--train_data\", \"training.json\", \"--val_data\",\n",
    "\"validation.json\"])\n",
    "\n",
    "    print(\"========== Loading data ==========\")\n",
    "    train_data, valid_data = load_data(args.train_data, args.val_data) # X_data is a list of pairs (document, y); y in {0,1,2,3,4}\n",
    "\n",
    "    # Think about the type of function that an RNN describes. To apply it, you will need to convert the text data into vector representations.\n",
    "    # Further, think about where the vectors will come from. There are 3 reasonable choices:\n",
    "    # 1) Randomly assign the input to vectors and learn better embeddings during training; see the PyTorch documentation for guidance\n",
    "    # 2) Assign the input to vectors using pretrained word embeddings. We recommend any of {Word2Vec, GloVe, FastText}. Then, you do not train/update these embeddings.\n",
    "    # 3) You do the same as 2) but you train (this is called fine-tuning) the pretrained embeddings further.\n",
    "    # Option 3 will be the most time consuming, so we do not recommend starting with this\n",
    "\n",
    "    print(\"========== Vectorizing data ==========\")\n",
    "    model = RNN(50, args.hidden_dim)  # Fill in parameters\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    word_embedding = pickle.load(open('./word_embedding.pkl', 'rb'))\n",
    "\n",
    "    stopping_condition = False\n",
    "    epoch = 0\n",
    "\n",
    "    last_train_accuracy = 0\n",
    "    last_validation_accuracy = 0\n",
    "\n",
    "    while not stopping_condition:\n",
    "        random.shuffle(train_data)\n",
    "        model.train()\n",
    "        # You will need further code to operationalize training, ffnn.py may be helpful\n",
    "        print(\"Training started for epoch {}\".format(epoch + 1))\n",
    "        train_data = train_data\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        minibatch_size = 16\n",
    "        N = len(train_data)\n",
    "\n",
    "        loss_total = 0\n",
    "        loss_count = 0\n",
    "        for minibatch_index in tqdm(range(N // minibatch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            loss = None\n",
    "            for example_index in range(minibatch_size):\n",
    "                input_words, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n",
    "                input_words = \" \".join(input_words)\n",
    "\n",
    "                # Remove punctuation\n",
    "                input_words = input_words.translate(input_words.maketrans(\"\", \"\", string.punctuation)).split()\n",
    "\n",
    "                # Look up word embedding dictionary\n",
    "                vectors = [word_embedding[i.lower()] if i.lower() in word_embedding.keys() else word_embedding['unk'] for i in input_words ]\n",
    "\n",
    "                # Transform the input into required shape\n",
    "                vectors = torch.tensor(vectors).view(len(vectors), 1, -1)\n",
    "                output = model(vectors)\n",
    "\n",
    "                # Get loss\n",
    "                example_loss = model.compute_Loss(output.view(1,-1), torch.tensor([gold_label]))\n",
    "\n",
    "                # Get predicted label\n",
    "                predicted_label = torch.argmax(output)\n",
    "\n",
    "                correct += int(predicted_label == gold_label)\n",
    "                # print(predicted_label, gold_label)\n",
    "                total += 1\n",
    "                if loss is None:\n",
    "                    loss = example_loss\n",
    "                else:\n",
    "                    loss += example_loss\n",
    "\n",
    "            loss = loss / minibatch_size\n",
    "            loss_total += loss.data\n",
    "            loss_count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss_total/loss_count)\n",
    "        print(\"Training completed for epoch {}\".format(epoch + 1))\n",
    "        print(\"Training accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
    "        \n",
    "        # Write results to output file\n",
    "        with open(\"training_results.out\", \"a\") as file:\n",
    "            file.write(f\"Epoch {epoch + 1} Training Accuracy: {correct / total}\\n\")\n",
    "            file.write(f\"Epoch {epoch + 1} Training Time: {time.time() - start_time}\\n\")\n",
    "            \n",
    "        trainning_accuracy = correct/total\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        random.shuffle(valid_data)\n",
    "        print(\"Validation started for epoch {}\".format(epoch + 1))\n",
    "        valid_data = valid_data\n",
    "\n",
    "        for input_words, gold_label in tqdm(valid_data):\n",
    "            input_words = \" \".join(input_words)\n",
    "            input_words = input_words.translate(input_words.maketrans(\"\", \"\", string.punctuation)).split()\n",
    "            vectors = [word_embedding[i.lower()] if i.lower() in word_embedding.keys() else word_embedding['unk'] for i\n",
    "                       in input_words]\n",
    "\n",
    "            vectors = torch.tensor(vectors).view(len(vectors), 1, -1)\n",
    "            output = model(vectors)\n",
    "            predicted_label = torch.argmax(output)\n",
    "            correct += int(predicted_label == gold_label)\n",
    "            total += 1\n",
    "            # print(predicted_label, gold_label)\n",
    "        print(\"Validation completed for epoch {}\".format(epoch + 1))\n",
    "        print(\"Validation accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
    "\n",
    "        # Write results to output file\n",
    "        with open(\"validation_results.out\", \"a\") as file:\n",
    "            file.write(f\"Epoch {epoch + 1} Validation Accuracy: {correct / total}\\n\")\n",
    "            file.write(f\"Epoch {epoch + 1} Validation Time: {time.time() - start_time}\\n\")\n",
    "            \n",
    "        validation_accuracy = correct/total\n",
    "\n",
    "        if validation_accuracy < last_validation_accuracy and trainning_accuracy > last_train_accuracy:\n",
    "            stopping_condition=True\n",
    "            print(\"Training done to avoid overfitting!\")\n",
    "            print(\"Best validation accuracy is:\", last_validation_accuracy)\n",
    "        else:\n",
    "            last_validation_accuracy = validation_accuracy\n",
    "            last_train_accuracy = trainning_accuracy\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "\n",
    "\n",
    "    # You may find it beneficial to keep track of training accuracy or training loss;\n",
    "\n",
    "    # Think about how to update the model and what this entails. Consider ffnn.py and the PyTorch documentation for guidance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
